{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509f6fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Could not determine script location via '__file__'. Assuming project root is one level up from current working directory: c:\\Users\\ADMIN\\Desktop\\KALEB\\10Academy\\Week2\\fintech-reviews-analysis\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1884/1884 [00:00<00:00, 2198.53it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251230cf8c3c4cbbb2adf761cc7b3a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\KALEB\\10Academy\\Week2\\fintech-reviews-analysis\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "ERROR:__main__:Failed to load Hugging Face model. Error: name 'torch' is not defined\n",
      "WARNING:__main__:Falling back to VADER sentiment analysis due to missing deep learning libraries.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1884/1884 [00:00<00:00, 3119.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1884/1884 [00:00<00:00, 2583.60it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 2: Sentiment and Thematic Analysis Pipeline\n",
    "\n",
    "This script loads the balanced and cleaned review data from Task 1,\n",
    "applies pre-trained Hugging Face DistilBERT for sentiment analysis,\n",
    "and extracts themes using a rule-based keyword approach.\n",
    "\n",
    "It aggregates the final results and saves an enriched CSV for reporting.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# We will use the 'emoji' library for conversion.\n",
    "# Ensure 'pip install emoji' is run if you use this code outside the current environment.\n",
    "try:\n",
    "    import emoji\n",
    "except ImportError:\n",
    "    # If the library is not available, we define a fallback function\n",
    "    def emojize(text, language='en', delimiters=(':', ':'), variant=None):\n",
    "        return text\n",
    "    logging.warning(\"The 'emoji' library is not installed. Emoji conversion will be skipped.\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# Safely determine the project root, accounting for environments where __file__ is not defined (like notebooks).\n",
    "try:\n",
    "    SCRIPT_PATH = os.path.abspath(__file__)\n",
    "    # Navigate three levels up from the script location (src/analysis/task_2_nlp_analysis.py -> PROJECT_ROOT)\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(SCRIPT_PATH)))\n",
    "except NameError:\n",
    "    # Fallback for environments where __file__ is not defined (like notebooks).\n",
    "    # Assumes the execution is happening one directory level below the project root (e.g., inside 'notebooks').\n",
    "    PROJECT_ROOT = os.path.dirname(os.getcwd()) \n",
    "    logging.warning(\n",
    "        \"Could not determine script location via '__file__'. Assuming project root is one level up from \"\n",
    "        f\"current working directory: {PROJECT_ROOT}\"\n",
    "    )\n",
    "\n",
    "DATA_PROCESSED_PATH = os.path.join(PROJECT_ROOT, 'data', 'processed')\n",
    "\n",
    "INPUT_FILENAME = \"final_bank_reviews_constrained.csv\"\n",
    "OUTPUT_FILENAME = \"reviews_with_sentiment_themes.csv\"\n",
    "AGGREGATED_FILENAME = \"aggregated_bank_insights.csv\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Enable progress bar for pandas apply/iteration\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- Thematic Keyword Definitions (Rule-Based Clustering) ---\n",
    "# These keywords and phrases are mapped to the 5 requested overarching themes.\n",
    "THEME_MAPPING = {\n",
    "    # 1. Account Access Issues\n",
    "    'Account Access Issues': [\n",
    "        'login error', 'cannot log', 'forgot password', 'pin', 'username', \n",
    "        'fingerprint', 'face id', 'access problem', 'locked out', 'security code', \n",
    "        'authentication', 'registration', 'session'\n",
    "    ],\n",
    "    # 2. Transaction Performance\n",
    "    'Transaction Performance': [\n",
    "        'slow', 'transfer fail', 'transaction fail', 'delay', 'stuck', \n",
    "        'pending', 'not delivered', 'speed', 'instantly', 'fast', \n",
    "        'loading', 'crash', 'bugs', 'down', 'lag'\n",
    "    ],\n",
    "    # 3. User Interface & Experience\n",
    "    'User Interface & Experience': [\n",
    "        'ui', 'user interface', 'design', 'layout', 'simple', 'confusing', \n",
    "        'easy to use', 'navigation', 'experience', 'complex', 'smooth', \n",
    "        'modern look', 'friendly', 'thumbs up', 'star' # Added 'thumbs up' and 'star'\n",
    "    ],\n",
    "    # 4. Customer Support\n",
    "    'Customer Support': [\n",
    "        'customer service', 'support team', 'call center', 'help desk', \n",
    "        'response', 'contact', 'reach out', 'fix', 'problem solved', 'unresponsive'\n",
    "    ],\n",
    "    # 5. Feature Requests & General\n",
    "    'Feature Requests & General': [\n",
    "        'wishlist', 'new feature', 'budgeting', 'saving goal', 'update', \n",
    "        'card management', 'virtual card', 'future', 'add', 'please include', 'thank you', 'love it' # Added general positive/request terms\n",
    "    ]\n",
    "}\n",
    "\n",
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads the final constrained review data.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')\n",
    "        df.reset_index(names=['review_id_generated'], inplace=True)\n",
    "        logger.info(f\"Loaded {len(df)} constrained reviews for NLP analysis.\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Input file not found: {filepath}. Run Task 1 preprocessing first.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "def convert_emojis(text: str) -> str:\n",
    "    \"\"\"Converts emojis in the text to their textual descriptions.\"\"\"\n",
    "    if 'emoji' in sys.modules:\n",
    "        # Replace emojis with their standard CLDR shortcodes (e.g., ðŸ‘ becomes :thumbs_up:)\n",
    "        text_with_shortcodes = emoji.demojize(text, delimiters=(\" :\", \": \"))\n",
    "        # Replace shortcodes with plain text (e.g., :thumbs_up: becomes thumbs_up)\n",
    "        return text_with_shortcodes.replace(\" :\", \" \").replace(\": \", \" \").replace(\":\", \"\")\n",
    "    return text\n",
    "\n",
    "def fallback_vader_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner) \n",
    "    as a robust fallback when the deep learning model (DistilBERT) fails to load.\n",
    "    \"\"\"\n",
    "    logger.warning(\"Falling back to VADER sentiment analysis due to missing deep learning libraries.\")\n",
    "    try:\n",
    "        # Attempt to download VADER lexicon data\n",
    "        nltk.download('vader_lexicon', quiet=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download NLTK VADER lexicon: {e}. Check internet connection/NLTK setup.\")\n",
    "        # We proceed anyway, VADER might still work if the lexicon is already present.\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def get_vader_sentiment(text):\n",
    "        \"\"\"Maps VADER compound score to POSITIVE, NEUTRAL, or NEGATIVE labels.\"\"\"\n",
    "        if pd.isna(text) or not text.strip():\n",
    "            return 'NEUTRAL', 0.0\n",
    "            \n",
    "        score = sia.polarity_scores(text)['compound']\n",
    "        \n",
    "        # Standard VADER classification thresholds\n",
    "        if score >= 0.05:\n",
    "            return 'POSITIVE', score\n",
    "        elif score <= -0.05:\n",
    "            return 'NEGATIVE', score\n",
    "        else:\n",
    "            return 'NEUTRAL', score\n",
    "\n",
    "    # Apply VADER analysis on the preprocessed text\n",
    "    vader_results = df['review_preprocessed'].progress_apply(get_vader_sentiment)\n",
    "    \n",
    "    # Unpack the results into the required columns\n",
    "    df[['sentiment_label', 'sentiment_score']] = pd.DataFrame(vader_results.tolist(), index=df.index)\n",
    "    \n",
    "    logger.info(\"Sentiment Analysis complete using VADER (Fallback).\")\n",
    "    return df\n",
    "\n",
    "def run_sentiment_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the preferred DistilBERT model. Falls back to VADER if the model cannot be loaded.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Pre-process the reviews: Convert emojis to text\n",
    "    df['review_preprocessed'] = df['review'].progress_apply(convert_emojis)\n",
    "    logger.info(\"Emoji conversion complete.\")\n",
    "    \n",
    "    # Initialize the sentiment analysis pipeline\n",
    "    try:\n",
    "        logger.info(\"Attempting to load DistilBERT sentiment model...\")\n",
    "        sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "        )\n",
    "        \n",
    "        # Process the preprocessed reviews\n",
    "        review_texts = df['review_preprocessed'].tolist()\n",
    "        \n",
    "        # Use tqdm to show progress during the time-consuming analysis step\n",
    "        results = sentiment_pipeline(tqdm(review_texts, desc=\"Analyzing Sentiment (DistilBERT)\"))\n",
    "        \n",
    "        # Extract results\n",
    "        df['sentiment_label'] = [res['label'] for res in results]\n",
    "        df['sentiment_score'] = [res['score'] for res in results]\n",
    "        \n",
    "        logger.info(\"Sentiment Analysis complete using DistilBERT.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        # This catches errors when PyTorch/TensorFlow are missing, or internet issues\n",
    "        logger.error(f\"Failed to load Hugging Face model. Error: {e}\")\n",
    "        # 2. Fallback to VADER\n",
    "        return fallback_vader_analysis(df)\n",
    "        \n",
    "\n",
    "def assign_theme(review_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Assigns a primary theme to a review based on keyword matching (Rule-Based Clustering).\n",
    "    The theme with the most matched keywords wins.\n",
    "    \"\"\"\n",
    "    if pd.isna(review_text):\n",
    "        return 'Unclassified'\n",
    "        \n",
    "    text_lower = review_text.lower()\n",
    "    \n",
    "    # Initialize score tracking for themes\n",
    "    theme_scores = {theme: 0 for theme in THEME_MAPPING.keys()}\n",
    "    \n",
    "    # Check for keyword matches\n",
    "    for theme, keywords in THEME_MAPPING.items():\n",
    "        for keyword in keywords:\n",
    "            # Use regex word boundary to match whole words/phrases accurately\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text_lower):\n",
    "                theme_scores[theme] += 1\n",
    "                \n",
    "    # Determine the theme with the highest score\n",
    "    # Filter out themes with zero scores\n",
    "    positive_scores = {theme: score for theme, score in theme_scores.items() if score > 0}\n",
    "    \n",
    "    if positive_scores:\n",
    "        # Get theme(s) with the maximum score\n",
    "        max_score = max(positive_scores.values())\n",
    "        best_themes = [theme for theme, score in positive_scores.items() if score == max_score]\n",
    "        \n",
    "        # If multiple themes have the same max score, return the first one found, \n",
    "        # or a composite label if you prefer, but sticking to one is simpler for initial analysis.\n",
    "        return best_themes[0]\n",
    "    else:\n",
    "        return 'General Feedback'\n",
    "\n",
    "def run_thematic_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the rule-based theme assignment to all reviews, using the preprocessed text.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting Rule-Based Thematic Analysis on preprocessed text...\")\n",
    "    \n",
    "    # Use the 'review_preprocessed' column (which has text instead of emojis) for theme assignment\n",
    "    df['identified_theme'] = df['review_preprocessed'].progress_apply(assign_theme)\n",
    "    \n",
    "    # Optional: Display top N-grams to verify theme keywords (not saved in DF)\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), stop_words='english', max_features=50)\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(df['review_preprocessed'])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        logger.info(f\"Top 10 keywords/n-grams (TF-IDF): {feature_names[:10].tolist()}\")\n",
    "    except ValueError:\n",
    "        logger.warning(\"Could not compute TF-IDF (empty vocabulary).\")\n",
    "\n",
    "\n",
    "    logger.info(\"Thematic Analysis complete.\")\n",
    "    return df\n",
    "\n",
    "def aggregate_insights(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates sentiment and theme data by bank and rating for high-level insights.\n",
    "    \"\"\"\n",
    "    logger.info(\"Aggregating insights by Bank and Rating...\")\n",
    "    \n",
    "    # 1. Prepare numerical representation for sentiment\n",
    "    # Map POSITIVE=1, NEUTRAL=0, NEGATIVE=-1 for a better average index calculation across all three labels\n",
    "    df['sentiment_numeric'] = df['sentiment_label'].map({'POSITIVE': 1, 'NEUTRAL': 0, 'NEGATIVE': -1}).fillna(0)\n",
    "    \n",
    "    # 2. Group and calculate key metrics\n",
    "    agg_df = df.groupby(['bank', 'rating']).agg(\n",
    "        total_reviews=('review', 'count'),\n",
    "        # Mean sentiment score: 1=Positive, 0=Neutral, -1=Negative\n",
    "        mean_sentiment_score=('sentiment_numeric', 'mean'), \n",
    "        median_rating=('rating', 'median'),\n",
    "        top_theme=('identified_theme', lambda x: x.mode()[0]) # Find the most frequent theme\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename for clarity\n",
    "    agg_df.rename(columns={'mean_sentiment_score': 'Avg_Sentiment_Index (-1 to 1)'}, inplace=True)\n",
    "    \n",
    "    logger.info(\"Aggregation complete. Resulting table shows key metrics per bank and rating.\")\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the NLP analysis pipeline.\"\"\"\n",
    "    input_filepath = os.path.join(DATA_PROCESSED_PATH, INPUT_FILENAME)\n",
    "    output_filepath = os.path.join(DATA_PROCESSED_PATH, OUTPUT_FILENAME)\n",
    "    aggregated_filepath = os.path.join(DATA_PROCESSED_PATH, AGGREGATED_FILENAME)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df = load_data(input_filepath)\n",
    "    if df.empty:\n",
    "        return \n",
    "        \n",
    "    # 2. Sentiment Analysis (Includes Emoji Conversion and VADER Fallback)\n",
    "    df_sentiment = run_sentiment_analysis(df)\n",
    "\n",
    "    # 3. Thematic Analysis (Keyword/Rule-Based Clustering)\n",
    "    # We rename it directly to df_final to keep the 'review_preprocessed' column\n",
    "    df_final = run_thematic_analysis(df_sentiment)\n",
    "    \n",
    "    # NOTE: The 'review_preprocessed' column is intentionally kept in df_final\n",
    "    # as requested, to allow comparison with the original 'review' column.\n",
    "\n",
    "    # 4. Aggregate Insights\n",
    "    df_aggregated = aggregate_insights(df_final.copy())\n",
    "    \n",
    "    # 5. Save Final Results\n",
    "    df_final.to_csv(output_filepath, index=False, encoding='utf-8')\n",
    "    logger.info(f\"ðŸ’¾ Saved enriched individual reviews to {output_filepath}\")\n",
    "    \n",
    "    df_aggregated.to_csv(aggregated_filepath, index=False, encoding='utf-8')\n",
    "    logger.info(f\"ðŸ’¾ Saved aggregated insights to {aggregated_filepath}\")\n",
    "\n",
    "    logger.info(\"\\nâœ¨ Task 2 Pipeline Complete. Data is ready for Visualization (Task 4) and Storage (Task 3).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c00442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Task 2: Numerical Analysis Results ---\n",
      "Data loaded from: c:\\Users\\ADMIN\\Desktop\\KALEB\\10Academy\\Week2\\fintech-reviews-analysis\\data\\processed\\aggregated_bank_insights.csv\n",
      "\n",
      "1. Average Overall Sentiment Index per Bank (Max 1.0, Min -1.0):\n",
      "| bank   |   Avg_Sentiment_Index (-1 to 1) |\n",
      "|:-------|--------------------------------:|\n",
      "| Dashen |                           0.351 |\n",
      "| CBE    |                           0.296 |\n",
      "| BOA    |                           0.237 |\n",
      "\n",
      "==================================================\n",
      "\n",
      "2. Critical (1-Star) Review Analysis:\n",
      "   Average Sentiment Index and Top Theme for 1-Star Reviews:\n",
      "\n",
      "| bank   |   1-Star Avg Sentiment |   1-Star Count | top_theme        |\n",
      "|:-------|-----------------------:|---------------:|:-----------------|\n",
      "| BOA    |                 -0.351 |            359 | General Feedback |\n",
      "| CBE    |                 -0.205 |            127 | General Feedback |\n",
      "| Dashen |                 -0.184 |             76 | General Feedback |\n",
      "\n",
      "==================================================\n",
      "\n",
      "3. Positive (5-Star) Review Driver Analysis:\n",
      "   Top Theme Driving 5-Star Satisfaction:\n",
      "\n",
      "| bank   |   5-Star Avg Sentiment |   5-Star Count | top_theme        |\n",
      "|:-------|-----------------------:|---------------:|:-----------------|\n",
      "| Dashen |                  0.852 |            338 | General Feedback |\n",
      "| CBE    |                  0.780 |            391 | General Feedback |\n",
      "| BOA    |                  0.675 |            228 | General Feedback |\n",
      "\n",
      "--- Numerical Summary Complete ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 2 Results Summary: Numerical Presentation\n",
    "\n",
    "This script reads the aggregated insights CSV generated by the Task 2 pipeline \n",
    "and prints key numerical findings to the console.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Safely determine the project root\n",
    "try:\n",
    "    SCRIPT_PATH = os.path.abspath(__file__)\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(SCRIPT_PATH)))\n",
    "except NameError:\n",
    "    # Fallback for environments where __file__ is not defined (like notebooks).\n",
    "    PROJECT_ROOT = os.path.dirname(os.getcwd()) \n",
    "\n",
    "DATA_PROCESSED_PATH = os.path.join(PROJECT_ROOT, 'data', 'processed')\n",
    "AGGREGATED_FILENAME = \"aggregated_bank_insights.csv\"\n",
    "INPUT_FILEPATH = os.path.join(DATA_PROCESSED_PATH, AGGREGATED_FILENAME)\n",
    "\n",
    "\n",
    "def display_numerical_results():\n",
    "    \"\"\"Reads the aggregated data and prints key numerical insights.\"\"\"\n",
    "    print(\"--- Task 2: Numerical Analysis Results ---\")\n",
    "    \n",
    "    try:\n",
    "        df_agg = pd.read_csv(INPUT_FILEPATH, encoding='utf-8')\n",
    "        \n",
    "        if df_agg.empty:\n",
    "            print(f\"Error: Aggregated data file is empty: {INPUT_FILEPATH}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Data loaded from: {INPUT_FILEPATH}\\n\")\n",
    "        \n",
    "        # 1. Overall Bank Performance Summary (Mean Sentiment Index)\n",
    "        overall_performance = df_agg.groupby('bank')['Avg_Sentiment_Index (-1 to 1)'].mean().sort_values(ascending=False)\n",
    "        print(\"1. Average Overall Sentiment Index per Bank (Max 1.0, Min -1.0):\")\n",
    "        print(overall_performance.to_markdown(floatfmt=\".3f\"))\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # 2. Critical Review (1-Star) Pain Points\n",
    "        df_1_star = df_agg[df_agg['rating'] == 1].sort_values(by='Avg_Sentiment_Index (-1 to 1)')\n",
    "        \n",
    "        print(\"2. Critical (1-Star) Review Analysis:\")\n",
    "        print(\"   Average Sentiment Index and Top Theme for 1-Star Reviews:\\n\")\n",
    "        \n",
    "        critical_insights = df_1_star[[\n",
    "            'bank', \n",
    "            'Avg_Sentiment_Index (-1 to 1)', \n",
    "            'total_reviews', \n",
    "            'top_theme'\n",
    "        ]].rename(columns={'Avg_Sentiment_Index (-1 to 1)': '1-Star Avg Sentiment', 'total_reviews': '1-Star Count'})\n",
    "        \n",
    "        print(critical_insights.to_markdown(index=False, floatfmt=\".3f\"))\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # 3. Top Theme for Positive Reviews (5-Star)\n",
    "        df_5_star = df_agg[df_agg['rating'] == 5].sort_values(by='Avg_Sentiment_Index (-1 to 1)', ascending=False)\n",
    "        \n",
    "        print(\"3. Positive (5-Star) Review Driver Analysis:\")\n",
    "        print(\"   Top Theme Driving 5-Star Satisfaction:\\n\")\n",
    "        \n",
    "        positive_insights = df_5_star[[\n",
    "            'bank', \n",
    "            'Avg_Sentiment_Index (-1 to 1)', \n",
    "            'total_reviews', \n",
    "            'top_theme'\n",
    "        ]].rename(columns={'Avg_Sentiment_Index (-1 to 1)': '5-Star Avg Sentiment', 'total_reviews': '5-Star Count'})\n",
    "        \n",
    "        print(positive_insights.to_markdown(index=False, floatfmt=\".3f\"))\n",
    "        print(\"\\n--- Numerical Summary Complete ---\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Aggregated file not found at {INPUT_FILEPATH}.\")\n",
    "        print(\"Please ensure you successfully ran the Task 2 NLP analysis script first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during analysis: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_numerical_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac73677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
